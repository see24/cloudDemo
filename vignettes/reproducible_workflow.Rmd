---
title: "Reproducible Project Workflow"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Reproducible Project Workflow}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  eval = FALSE,
  comment = "#>"
)
```


A reproducible project contains all the code, data, package versions and operating system information needed to run the analysis from scratch on a new computer. In this tutorial we will explore the benefits of adopting a standard project structure, using the structure of an R package and writing papers along side code. We will incorporate tools from the packages [rmarkdown](https://rmarkdown.rstudio.com/), [rcompendium](https://frbcesab.github.io/rcompendium/) and [usethis](https://usethis.r-lib.org/). Using Git and GitHub are key parts of this reproducible workflow. If they are new to you see my tutorial on [Git and GitHub for Reproducible Research](https://landscitech.github.io/Github_tutorial/). 

## Setting up your project

To get a standard folder structure across repositories I have created a template repository on GitHub that I use to create a new repository and RStudio project at the beginning of a new project. To create a new repository from the template click the green "Use this template" button on the top right of the template repository [page](https://github.com/LandSciTech/paper-template-repo). Follow the instructions to give your repository a name and chose the organization and visibility. Note this template is an amalgam of the one used by rcompendium and [researchcompendium](https://github.com/benmarwick/researchcompendium/tree/master). 

Then in RStudio run the following replacing the url with the one from your newly created repository and choosing your desired path.

```{r}
usethis::create_from_github("https://github.com/see24/test_repo.git", destdir = "~/Desktop")
```

The folder structure from the template is as follows:

- `DESCRIPTION`: contains project metadata (authors, date, dependencies, etc.)

- `make.R`: main R script to run the entire project

- `R/`: contains R functions developed for this project

- `man/`: contains documentation for those functions

- `analyses/`: contains source code for the paper and scripts that are part of the analysis but not run in the paper. Also includes templates that are used to render the paper. 

- `data/`: contains data needed to run the project. raw-data should contain unmodified source data, derived-data should contain modified versions of the data

- `figures/`: contains figure files including those generated by rendering the paper

Having a consistent folder structure across projects makes it easy to remember the correct paths to use and to find data across various projects. One of the keys of reproducible code is using relative paths where the project folder is the starting point of all paths. For example, I might use "data/raw-data/mydata.csv". If all data and scripts are within the project folder we never need to use [`setwd()`](https://www.tidyverse.org/blog/2017/12/workflow-vs-script/). 

We now need to modify the template to use our package name and fill in placeholders with real values.

1. Use Ctrl-Shift-F to find all uses of PackageName and replace it with the name of the repo you created. If you are not part of LandSciTech first replace LandSciTech/PackageName with your GitHub user name and then replace PackageName.
1. Rename the .Rproj file to have the same name as the repo.
1. Fill in metadata in the DESCRIPTION.
1. update the README.Rmd and knit it.

Additional details can be added to the README and DESCRIPTION once the project is more mature. 

## Adopting the R package structure

The template follows the structure of an R package with functions in `R/`, function documentation in `man/` and `DESCRIPTION`, `NAMESPACE`, and .Rbuildignore files. If these are unfamiliar don't worry! They are not as scary as they seem and allow for some very useful workflows. Next we will work through a small example of finding the package within a script. 

Create a blank R script in the analyses folder with a number to indicate the order it should be run in eg "analyses/01_run_model.R". Our goal in this analysis is to determine which variable in the mtcars data set is the best predictor of mpg. To do this we build a linear model for each variable vs mpg. A first attempt might be to right them all out by hand.

```{r}
  mod_cyl <- lm(mpg ~ cyl, data = mtcars)
  gl_cyl <- broom::glance(mod_cyl)
  
  mod_disp <- lm(mpg ~ disp, data = mtcars)
  gl_disp <- broom::glance(mod_disp)
  
  data.frame(var = c("cyl", "disp"), r.squared = c(gl_cyl$r.squared, gl_disp$r.squared))
  # ... for all the variables... sigh
```

But it will be much faster and harder to make mistakes if we use a function and iterate over each variable name. 

```{r}
do_mod <- function(x){
  form <- as.formula(paste0("mpg ~ ", x))
  mod <- lm(form, data = mtcars)
  gl <- broom::glance(mod)
  
  data.frame(variable = x, r.squared = gl$r.squared)
}

r2_tab <- purrr::map_dfr(names(mtcars)[-1], do_mod) |> 
  dplyr::mutate(variable = reorder(variable, r.squared))

r2_tab
```

Here I have used `packagename::function()` instead of loading the whole package for purrr, dplyr and broom because it is more explicit and makes it clear where each function comes from. For every new package you use in a project it is helpful to record the package in the DESCRIPTION by calling `usethis::use_package("packagename")`. If you want to avoid typing the package name you have two options. First, you can do it the package way and import the function you need with `usethis::use_import_from("packagname", "function")` which adds `#' @importFrom package function` to the package documentation file ("R/testCompendium-package.R"). If you want the whole package to be loaded add `#' @import package` to the same file and then run `devtools::document()`. This adds them to the NAMESPACE files and  ensures that the scripts work the same anytime the project is loaded with `devtools::load_all()` rather than being sensitive to which `library(package)` calls have happened and in which order. The second option is to use `library(package)` in your scripts which will usually still work but may cause issues if you use your functions in another project.   

Having the function definition in the R script is fine for this simple function but if we want to use the function in multiple scripts or take advantage of R package development tools to automatically load all our functions and have the option to test and document those functions we need to put the function definition in the R folder. You can create a new file manually called "R/do_mod.R" or you can use `usethis::use_r("do_mod")` to create and open the file. Then copy the function definition to the file. Next we want to document the function so that the inputs and outputs and any assumptions of the function are clear. To do so using roxygen2 comments click inside the function definition and then click "Code > Insert Roxygen Skeleton" from the RStudio menu. Fill in the title followed by a description of what the function does and then a description of each parameter and the value that is returned. Now run `devtools::document()` and then ?do_mod to see the documentation for the function. See the R packages chapter [Function Documentation](https://r-pkgs.org/man.html) for details. 

```{r}
#' Run the model for any variable
#'
#' Run a linear model of the variable vs mpg for the mtcars data
#' set and extract the r squared
#'
#' @param x variable name from mtcars
#'
#' @return data.frame with the variable name and r.squared value
#'
#' @export
#' 
do_mod <- function(x){
  form <- as.formula(paste0("mpg ~ ", x))
  mod <- lm(form, data = mtcars)
  gl <- broom::glance(mod)
  
  data.frame(variable = x, r.squared = gl$r.squared)
}

```

Now if you run `devtools::load_all()` all the code in `R/` will be sourced and the functions will be available as if it was a package that you loaded. Also packages listed with `# @imports` or `# @importFrom` will be loaded.  

At this stage we should have two scripts "R/do_mod.R" that contains the function definition and "analyses/01_run_model.R" with the code to analyze the mtcars data. Now we can add another script to the analyses folder where we will make a figure of our results, "analyses/02_make_fig.R". And save the following code. I also add `# @imports ggplot2` to the "R/testCompendium-package.R" script so that I don't need to write `ggplot2::` all the time.

```{r}
ggplot(r2_tab, aes(variable, r.squared))+
  geom_col()

ggsave("figures/r2_mpg_variables.png")
```

Once the scripts are ready I add the following to the bottom of the "make.R" file that was created from the template. This will source each script and also set a global ggplot2 theme. 

```{r}
## Global Variables ----

# You can list global variables here (or in a separate R script)

# set a global ggplot theme
theme_set(theme_classic())

## Run Project ----

# List all R scripts in a sequential order and using the following form:
source(here::here("analyses", "01_run_model.R"))
source(here::here("analyses", "02_make_fig.R"))
```

The make.R file also runs `devtools::install_deps(upgrade = "never")` which will install the R packages listed in the DESCRIPTION. To ensure that all dependencies have been recorded in the DESCRIPTION run `rcompendium::add_dependencies(".")`.

Finally, restart R and then run `source("make.R")`. This will ensure that your scripts work from a fresh R session. Then commit and push all your changes to GitHub. Now a collaborator should be able to clone your repo from GitHub and run `source("make.R")` to reproduce your analysis. 

## Managing data

Above we used a built in data set but typically you will have some data that you want to store in the data folder. If the data is less than 100 Mb and is recorded in a text or csv file it can be tracked in GitHub and can be included in the repo. Otherwise you will want to store the data somewhere other than GitHub. 

To stop git from tracking things stored in the data folder you can add "data/" to a new line in the .gitignore file in the project directory (`usethis::edit_git_ignore("project")` will open the file for you). This means that the data folder will not be tracked by git and will not be uploaded to the repo on GitHub so you will need to have an alternative mechanism for collaborators to access the data. The simplest option is to add a note in the project README that explains where to manually download the data or who to contact to get the data. To have a more automated solution you will need to store the data somewhere online. Two options with R packages are Google Drive ([googledrive](https://googledrive.tidyverse.org/)) and OSF ([osfr](https://docs.ropensci.org/osfr/)). To use googledrive for example you would need to have the data available in a Google Drive folder that is shared with the user. Then you can include something like the below as the first step in your data analysis. 

```{r}
# get raw lit review data from google drive
gd_url <- "https://docs.google.com/spreadsheets/d/16X9oxbmufCTy8WpV2zSj9KoVmImkXa5Mz-KGGMBqIkk/edit?usp=sharing"

# file path where data is saved
fl_pth <- here::here("data/raw-data", "data_from_GD.xlsx")

# running this will prompt you for permission to connect to your google drive
googledrive::drive_auth(email = TRUE)

googledrive::drive_download(
  googledrive::as_id(gd_url),
  path = fl_pth,
  overwrite = TRUE)

raw_lr_dat <- readxl::read_xlsx(fl_pth)
```

OSF (Open Science Framework) is better than Google drive if you want a more polished place to store data that you will publish. Creating an OSF project creates a private or public website that can be used with collaborators or the public. For an example see the OSF page for the [CAN-SAR Database](https://osf.io/e4a58/). You can archive an OSF project with a doi to reference a specific version in a paper (called a "registration"). 

Another option for sharing data with ECCC collaborators is to use OneDrive. OneDrive does not play nicely with Git or R projects typically so I have found a work around to allow OneDrive to track only the data folder of a project. If you create a folder in OneDrive for the data and then create a symbolic link inside
an R project you can put data there and it will be stored and synced to
the OneDrive cloud.

#### To share your folder

1)  Make a OneDrive folder called NOBMWG_data

2)  rename your existing data folder to data2

3)  In command prompt launched as administrator run:

```
mklink /d c:\Users\USERNAME\Documents\path_to_project\data "c:\Users\USERNAME\OneDrive - EC-EC\ProjectName_data"
```

modifying paths as needed. This will create a shortcut in your
project folder but you can use the path in your R scripts as if
it was a regular folder.

4)  Copy the data into the newly created folder/shortcut named data

5)  Once you feel confident the data is stored securely delete data2

6) Add \*data\* to your .gitignore file because the symlink is not seen as a folder


#### To get a folder that someone else has shared:

1)  Open the folder location in OneDrive
    <https://007gc-my.sharepoint.com/:f:/g/personal/sarah_endicott_ec_gc_ca/EqYYHNFZff1NmmyDQneHSEQBeCWCodKIFoKNG6nujdw_sw?email=Josie.Hughes%40ec.gc.ca&e=Bh2CmB>
    
2)  Click the 3 dots and select Add shortcut to My Files. Confirm that
    the link is available in your OneDrive folder in Windows File
    Explorer.
    
3)  Launch the command prompt as an administrator and run the following:

```
mklink /d c:\Users\USERNAME\Documents\path_to_project\data "c:\Users\USERNAME\OneDrive - EC-EC\ProjectName_data"
```
4) Add \*data\* to your .gitignore file because the symlink is not seen as a folder

modifying paths as needed. This will create a shortcut in your
project folder but you can use the path in your R scripts as if
it was a regular folder.


## Writing a paper
You can use Rmarkdown or Quarto to author papers. The main benefit is the ability to include figures, tables and in text code in the paper so that you never need to update numbers or copy paste figures in a paper after revising the analysis. It also automatically formats citations based on a csl file so you can change citation formats quickly by just changing files. RStudio also [connects](https://rstudio.github.io/visual-markdown-editing/citations.html) to Zotero to allow you to insert citations directly from a Zotero library. 

Writing R markdown documents has a bit of a learning curve but the analyses/paper/paper.Rmd file in the template repo is a good starting point that includes examples of tables, figures, citations, equations and more! In addtion, Software Carpentry has a detailed tutorial [available](https://ucsbcarpentry.github.io/R-markdown/aio/index.html)




